---
title: "Mid America Oireachtas"
output: html_document
date: '2022-05-09'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
library(stringr)
```

# Getting the Results

The first thing to do is get the results into R. Unfotunaly all of the result files are PDFs so I'll have a fair amount of tidying to do once they are in.  

The results for the Mid America Region from 2011 to present are at 
[QuickFeis](https://www.quickfeis.com/Oireachtas). Each year has a page and then a button for each individual competition which will download the pdf. Since I don't want to spend hours clicking to open and save all the individual files I'm going to find a way to automate at least a good portion of the process. 

```{r}
page <- read_html("https://www.quickfeis.com/Oireachtas")

raw_list <- page %>% 
    html_elements("a") %>%                  #find the links in the page
    html_attr("href") %>%                   #get the url for the links
    str_subset("FullOireachtasResultsFree")  #%>% #find only the Full Results links

# problem. They have stupid placeholders need to figure out a way to just get the part in line 34 - maybe some sort of extract between?
# <a id="ContentPlaceHolderBody_LinkButton17" href="javascript:WebForm_DoPostBackWithOptions(new WebForm_PostBackOptions(&quot;ctl00$ContentPlaceHolderBody$LinkButton17&quot;, &quot;&quot;, false, &quot;&quot;, &quot;
# FullOireachtasResultsFree.aspx?OireachtasId=14
# &quot;, false, true))">Full Results</a>



#    str_c("https://www.quickfeis.com/", .) #%>% #prepend website to the url
#     map(read_html) %>%
#     map(html_element, "#raw-url") %>%
#     map(html_attr, "href") %>%
#     str_c("https://www.quickfeis.com/", .) %>%
#     walk2(., basename(.), download.file, mode = "wb")




print(raw_list)

```


```{r}
page2 <- read_html("https://github.com/rstudio/cheatsheets")

raw_list2 <- page2 %>% # takes the page above for which we've read the html
    html_nodes("a") %>% # find all links in the page 
    html_attr("href") %>% # get the url for these links 
    str_subset(".pdf") %>% # find those that end in pdf only
    str_c("https://www.github.com", .) %>% # prepend the website to the url
    map(read_html) %>% # take previously generated list of urls and read them
    map(html_node, "#raw-url") %>% # parse out the 'raw' url - the link for the download button 
    map(html_attr, "href") %>% # return the set of raw urls for the download buttons 
    str_c("https://www.github.com", .) %>% # prepend the website again to get a full url 
    walk2(., basename(.), download.file, mode = "wb") # use purrr to download the pdf associated with each url to the current working directory




print(raw_list2)
```

```{r}
fruit <- c("apple", "banana", "pear", "pinapple")
str_detect(fruit, "a")
str_detect(fruit, "^a")
str_detect(fruit, "a$")
str_detect(fruit, "b")
str_detect(fruit, "[aeiou]")

str_subset(fruit, "b")


# Also vectorised over pattern
str_detect("aecfg", letters)

# Returns TRUE if the pattern do NOT match
str_detect(fruit, "^p", negate = TRUE)

```

